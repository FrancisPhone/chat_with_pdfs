{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7525a16",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8048d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "DATA_DIR = 'papers/'\n",
    "VECTOR_STORE_PATH = 'chroma_db/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14112b7",
   "metadata": {},
   "source": [
    "# Ingest PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46683257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "debb0821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdfs():\n",
    "    documents = []\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(DATA_DIR, filename))\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    Chroma.from_documents(splits, embeddings, persist_directory=VECTOR_STORE_PATH)\n",
    "    print(\"PDFs ingested successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3beb7c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs ingested successfully.\n"
     ]
    }
   ],
   "source": [
    "ingest_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85194d8",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7923f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self):\n",
    "        self.sessions = defaultdict(list)\n",
    "\n",
    "    def add_message(self, session_id: str, message: dict):\n",
    "        self.sessions[session_id].append(message)\n",
    "\n",
    "    def get_session_history(self, session_id: str) -> list:\n",
    "        return self.sessions.get(session_id, [])\n",
    "\n",
    "    def clear_session(self, session_id: str):\n",
    "        if session_id in self.sessions:\n",
    "            del self.sessions[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554f5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemoryManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebdd14",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb385c",
   "metadata": {},
   "source": [
    "## Clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97aeb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def clarify_query(query: str, session_id: str, llm: ChatOpenAI, memory: MemoryManager) -> str:\n",
    "    history = memory.get_session_history(session_id)\n",
    "    prompt = f\"\"\"\n",
    "    Conversation history: {history}\n",
    "    Analyze the query: \"{query}\"\n",
    "    If it is vague or underspecified (e.g., missing context or unclear terms), return a clarified version or a question by looking at the conversation history to ask the chatbot.\n",
    "    If it is clear, return the original query.\n",
    "    Example: hisory: \"OCR-Free Document Understanding Transformer\" paper \n",
    "             query: \"What is the accuracy?\"\n",
    "             clarified_query: \"What is the accuracy of the model in the 'OCR-Free Document Understanding Transformer' paper?\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17e486",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d3f0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def route_query(query: str, llm: ChatOpenAI) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Determine if the query \"{query}\" can be answered using academic papers or requires a web search.\n",
    "    Return \"pdf\" for queries related to academic papers, or \"web\" for queries requiring external information.\n",
    "    Example: \"What did OpenAI release this month?\" -> \"web\"\n",
    "    Example: \"Which prompt template gave the highest zero-shot accuracy in Zhang et al.?\" -> \"pdf\"\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42aed0",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fafe72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma  \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def retrieve_and_answer(query: str, llm: ChatOpenAI) -> str:\n",
    "    vector_store = Chroma(persist_directory=VECTOR_STORE_PATH, embedding_function=OpenAIEmbeddings())\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    docs = retriever.invoke(query) \n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"context\"],\n",
    "        template=\"Based on the context: {context}\\nAnswer the query: {query}\"\n",
    "    )\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    response = llm.invoke(prompt.format(query=query, context=context))\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b0bb4",
   "metadata": {},
   "source": [
    "# Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14ce2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def web_search(query: str, llm: ChatOpenAI, tavily: TavilyClient) -> str:\n",
    "    results = tavily.search(query=query, max_results=3)\n",
    "    context = \"\\n\".join([result[\"content\"] for result in results[\"results\"]])\n",
    "    prompt = f\"Based on the web search results: {context}\\nAnswer the query: {query}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf055",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13179c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def generate_answer(query: str, context: str, session_id: str, llm: ChatOpenAI, memory: MemoryManager) -> str:\n",
    "    history = memory.get_session_history(session_id)\n",
    "    prompt = f\"\"\"\n",
    "    Conversation history: {history}\n",
    "    Context: {context}\n",
    "    Query: {query}\n",
    "    Provide a concise and accurate answer.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    memory.add_message(session_id, {\"query\": query, \"answer\": response.content})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f3441",
   "metadata": {},
   "source": [
    "# Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a63dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16f8c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    session_id: str\n",
    "    web_search: bool\n",
    "\n",
    "class State(Dict[str, Any]):\n",
    "    query: str\n",
    "    session_id: str\n",
    "    clarified_query: str\n",
    "    route_decision: str\n",
    "    context: str\n",
    "    response: str\n",
    "    web_search: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4183a",
   "metadata": {},
   "source": [
    "# Lang Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc92215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fac9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c73c6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarification_node(state: State) -> State:\n",
    "    state[\"clarified_query\"] = clarify_query(state[\"query\"], state[\"session_id\"], llm, memory)\n",
    "    return state\n",
    "\n",
    "def routing_node(state: State) -> State:\n",
    "    if state[\"web_search\"]:\n",
    "        state[\"route_decision\"] = 'web'\n",
    "    else:\n",
    "        state[\"route_decision\"] = route_query(state[\"clarified_query\"], llm)\n",
    "    return state\n",
    "\n",
    "def retrieval_node(state: State) -> State:\n",
    "    if state[\"route_decision\"] == \"pdf\":\n",
    "        state[\"context\"] = retrieve_and_answer(state[\"clarified_query\"], llm)\n",
    "    else:\n",
    "        state[\"context\"] = web_search(state[\"clarified_query\"], llm, tavily)\n",
    "    return state\n",
    "\n",
    "def answer_node(state: State) -> State:\n",
    "    state[\"response\"] = generate_answer(state[\"clarified_query\"], state[\"context\"], state[\"session_id\"], llm, memory)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d73e2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"clarify\", clarification_node)\n",
    "workflow.add_node(\"route\", routing_node)\n",
    "workflow.add_node(\"retrieve\", retrieval_node)\n",
    "workflow.add_node(\"answer\", answer_node)\n",
    "workflow.add_edge(\"clarify\", \"route\")\n",
    "workflow.add_edge(\"route\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"answer\")\n",
    "workflow.add_edge(\"answer\", END)\n",
    "workflow.set_entry_point(\"clarify\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b26e5",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79bf1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear_session(\"user1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ff988f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"How to Prompt LLMs for Text-to-SQL\" presents several key findings and contributions:\n",
      "\n",
      "1. **Comparative Analysis of Prompt Constructions**: It highlights the variability in prompt strategies from previous research and their impact on LLM performance in text-to-SQL tasks.\n",
      "\n",
      "2. **Optimal Prompt Template Identification**: The authors identify the best prompt template by partitioning prompts into components and testing their effectiveness in enhancing LLM performance.\n",
      "\n",
      "3. **Benchmarking Across Various LLMs**: The research benchmarks a range of LLMs, including general-purpose and coding-specific models, to evaluate performance differences and boundaries.\n",
      "\n",
      "4. **Impact of Information Granularity**: It assesses how the level of detail in prompts affects model performance and identifies optimal learning strategies, such as zero-shot and few-shot approaches.\n",
      "\n",
      "Overall, the paper aims to improve the generation of SQL queries by optimizing prompt strategies and understanding their influence on model performance.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"Do you know about 'How to Prompt LLMs for Text-to-SQL' paper?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a5214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Do you know about 'How to Prompt LLMs for Text-to-SQL' paper?\",\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"What are the main findings or contributions of the paper \\'How to Prompt LLMs for Text-to-SQL\\'?\"',\n",
       " 'route_decision': 'pdf',\n",
       " 'context': 'The paper \"How to Prompt LLMs for Text-to-SQL\" presents several key findings and contributions:\\n\\n1. **Comparative Analysis of Prompt Constructions**: The study investigates various prompt strategies employed in previous research, highlighting the lack of comparability in prompt constructions and their contributions to LLM performance in text-to-SQL tasks.\\n\\n2. **Optimal Prompt Template Identification**: The authors partition prompt text into distinct components and conduct thorough testing to determine the optimal prompt template for enhancing LLM performance on end-to-end text-to-SQL tasks.\\n\\n3. **Benchmarking Across Various LLMs**: The research includes a benchmarking approach that evaluates a range of LLMs, encompassing both general-purpose and coding-specific models of varying parameter sizes, to understand their performance boundaries and disparities.\\n\\n4. **Impact of Information Granularity**: The study systematically assesses how the granularity of information in prompts affects model performance and identifies optimal context learning strategies, such as zero-shot and few-shot approaches, to maximize performance.\\n\\nOverall, the paper aims to enhance the effectiveness of LLMs in generating SQL queries by optimizing prompt strategies and understanding their influence on model performance.',\n",
       " 'response': 'The paper \"How to Prompt LLMs for Text-to-SQL\" presents several key findings and contributions:\\n\\n1. **Comparative Analysis of Prompt Constructions**: It highlights the variability in prompt strategies from previous research and their impact on LLM performance in text-to-SQL tasks.\\n\\n2. **Optimal Prompt Template Identification**: The authors identify the best prompt template by partitioning prompts into components and testing their effectiveness in enhancing LLM performance.\\n\\n3. **Benchmarking Across Various LLMs**: The research benchmarks a range of LLMs, including general-purpose and coding-specific models, to evaluate performance differences and boundaries.\\n\\n4. **Impact of Information Granularity**: It assesses how the level of detail in prompts affects model performance and identifies optimal learning strategies, such as zero-shot and few-shot approaches.\\n\\nOverall, the paper aims to improve the generation of SQL queries by optimizing prompt strategies and understanding their influence on model performance.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c11788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"How to Prompt LLMs for Text-to-SQL\" uses the following metrics to measure the quality of SQL queries:\n",
      "\n",
      "1. **F1 values of Rouge-1/2/L** - Evaluates the overlap between generated SQL and expected SQL based on n-grams.\n",
      "2. **BertScore** - Assesses the quality of generated SQL by comparing it to reference texts using contextual embeddings.\n",
      "3. **Semantic coherence assessment** - Utilizes LLMs to evaluate the semantic coherence between generated SQL statements and original natural language questions.\n",
      "\n",
      "These metrics aim to assess both the accuracy and semantic understanding of the generated SQL queries.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"What metrics are used to measure the SQL quality in that paper?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee658614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What metrics are used to measure the SQL quality in that paper?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"What metrics are used to measure the SQL quality in the paper \\'How to Prompt LLMs for Text-to-SQL\\'?\"',\n",
       " 'route_decision': 'pdf',\n",
       " 'context': 'The paper \"How to Prompt LLMs for Text-to-SQL\" uses several metrics to measure the quality of SQL queries. These metrics include:\\n\\n1. **F1 values of Rouge-1/2/L** - These metrics evaluate the overlap between the generated SQL query and the expected SQL query in terms of n-grams.\\n\\n2. **BertScore** - This metric assesses the quality of generated text by comparing it to reference texts using contextual embeddings.\\n\\n3. **Semantic coherence assessment** - The paper also utilizes LLMs to evaluate the semantic coherence between the generated SQL statements and the original natural language questions.\\n\\nThese evaluation metrics are designed to assess the accuracy and semantic understanding of the SQL generated from natural language queries.',\n",
       " 'response': 'The paper \"How to Prompt LLMs for Text-to-SQL\" uses the following metrics to measure the quality of SQL queries:\\n\\n1. **F1 values of Rouge-1/2/L** - Evaluates the overlap between generated SQL and expected SQL based on n-grams.\\n2. **BertScore** - Assesses the quality of generated SQL by comparing it to reference texts using contextual embeddings.\\n3. **Semantic coherence assessment** - Utilizes LLMs to evaluate the semantic coherence between generated SQL statements and original natural language questions.\\n\\nThese metrics aim to assess both the accuracy and semantic understanding of the generated SQL queries.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c68761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there is GitHub code available for the paper \"How to Prompt LLMs for Text-to-SQL.\" You can find the repository [here](https://github.com/shuaichenchang/prompt-text-to-sql).\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"Is there any github code for that paper?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58cf98ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Is there any github code for that paper?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"Is there any GitHub code available for the paper \\'How to Prompt LLMs for Text-to-SQL\\'?\"',\n",
       " 'route_decision': 'web',\n",
       " 'context': 'Yes, there is GitHub code available for the paper \"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings.\" You can find the repository [here](https://github.com/shuaichenchang/prompt-text-to-sql). The repository includes Python scripts like `text_to_sql.py`, which you can use to run text-to-SQL tasks with various settings and models. For example, you can run it with the Codex model in a zero-shot setting using the command:\\n\\n```bash\\npython text_to_sql.py --setting zeroshot --model codex --prompt_db \"CreateTableSelectCol\"\\n```',\n",
       " 'response': 'Yes, there is GitHub code available for the paper \"How to Prompt LLMs for Text-to-SQL.\" You can find the repository [here](https://github.com/shuaichenchang/prompt-text-to-sql).'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b44bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In general, a few hundred examples are often considered sufficient to achieve good accuracy in many machine learning tasks, though this can vary based on factors such as task complexity, data diversity, and model architecture. In few-shot learning scenarios, even 5 to 10 examples can improve performance for adaptable models like Codex. However, there is a threshold beyond which additional examples may lead to diminishing returns.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"How many examples are enough for good accuracy\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be1b0d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How many examples are enough for good accuracy',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"In the context of training a machine learning model, how many examples are generally considered enough to achieve good accuracy?\"',\n",
       " 'route_decision': 'pdf',\n",
       " 'context': 'In the context of training a machine learning model, the number of examples considered enough to achieve good accuracy can vary significantly based on several factors, including the complexity of the task, the diversity of the data, and the specific model architecture being used. \\n\\nFrom the information provided, we can see examples from the Scholar and GeoQuery datasets. The finetuned T5 model achieved 87.2% accuracy with 499 examples for Scholar and 85.7% accuracy with 549 examples for GeoQuery. These figures suggest that a few hundred examples can yield competitive accuracy for certain tasks, particularly in the context of text-to-SQL systems.\\n\\nIn few-shot learning scenarios, it appears that even a small number of examples (e.g., 5 to 10) can lead to improved performance, particularly with models like Codex, which adapt well to limited data. However, there is a threshold beyond which additional examples might lead to diminishing returns or even decreased performance due to issues like ambiguity.\\n\\nIn summary, while there is no one-size-fits-all answer, a few hundred examples are often sufficient for achieving good accuracy in many machine learning tasks, with variations depending on the specific application and model characteristics.',\n",
       " 'response': 'In general, a few hundred examples are often considered sufficient to achieve good accuracy in many machine learning tasks, though this can vary based on factors such as task complexity, data diversity, and model architecture. In few-shot learning scenarios, even 5 to 10 examples can improve performance for adaptable models like Codex. However, there is a threshold beyond which additional examples may lead to diminishing returns.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d988534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt template that yielded the highest zero-shot accuracy on the Spider dataset according to the study by Zhang et al. (2024) is \"Create Table + Select 3.\"\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"Which prompt template gave the highest zero-shot accuracy on Spider in Zhang et al. (2024)?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84214a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Which prompt template gave the highest zero-shot accuracy on Spider in Zhang et al. (2024)?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"Which prompt template yielded the highest zero-shot accuracy on the Spider dataset according to the study by Zhang et al. (2024)?\"',\n",
       " 'route_decision': 'pdf',\n",
       " 'context': 'Based on the information provided, the highest zero-shot accuracy on the Spider dataset according to the study is achieved by the prompt template \"Create Table + Select 3.\" This combination yielded the best results in terms of valid SQL predictions, execution accuracy, and test-suite accuracy for the Codex model.',\n",
       " 'response': 'The prompt template that yielded the highest zero-shot accuracy on the Spider dataset according to the study by Zhang et al. (2024) is \"Create Table + Select 3.\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63b9ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Davinci Codex model achieves an execution accuracy of 67.0% on the Spider dataset using the \"Create Table + Select 3\" prompt.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"What execution accuracy does davinci-codex reach on Spider with the ‘Create Table + Select 3’ prompt?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40f599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What execution accuracy does davinci-codex reach on Spider with the ‘Create Table + Select 3’ prompt?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"What execution accuracy does the Davinci Codex model achieve on the Spider dataset using the \\'Create Table + Select 3\\' prompt?\"',\n",
       " 'route_decision': 'pdf',\n",
       " 'context': 'The Davinci Codex model achieves an execution accuracy of 67.0% on the Spider dataset using the \"Create Table + Select 3\" prompt.',\n",
       " 'response': 'The Davinci Codex model achieves an execution accuracy of 67.0% on the Spider dataset using the \"Create Table + Select 3\" prompt.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2939731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In October 2023, OpenAI announced the upcoming release of GPT-5 and discussed the new ChatGPT Agent feature, indicating that user feedback from these agents may contribute to the model's training. Additionally, they have been gradually rolling out long-term memory in ChatGPT to enhance its ability to remember user information.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"What did OpenAI release this month?\", \"session_id\": \"user1\", \"web_search\": False})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "180c791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did OpenAI release this month?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"What specific release or announcement did OpenAI make this month?\"',\n",
       " 'route_decision': 'web',\n",
       " 'context': 'In October 2023, OpenAI made significant announcements regarding the development of GPT-5, as mentioned by Xikun Zhang during a discussion of the new ChatGPT Agent feature. This announcement indicated that GPT-5 \"is coming,\" and there are expectations that user feedback from ChatGPT Agents may contribute to its training. Additionally, OpenAI has been gradually rolling out long-term memory in ChatGPT, enhancing its ability to remember user information, which might be further improved with the release of GPT-5.',\n",
       " 'response': \"In October 2023, OpenAI announced the upcoming release of GPT-5 and discussed the new ChatGPT Agent feature, indicating that user feedback from these agents may contribute to the model's training. Additionally, they have been gradually rolling out long-term memory in ChatGPT to enhance its ability to remember user information.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e740d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is expected to release GPT-5 and a new open-weights model in August 2025.\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"query\": \"What did OpenAI release in 2025?\", \"session_id\": \"user1\", \"web_search\": True})\n",
    "print(state['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d90864a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did OpenAI release in 2025?',\n",
       " 'session_id': 'user1',\n",
       " 'clarified_query': 'clarified_query: \"What specific product or technology did OpenAI release in 2025?\"',\n",
       " 'route_decision': 'web',\n",
       " 'context': 'Based on the information provided, OpenAI is expected to release its next big model, GPT-5, in August 2025. Additionally, there is mention of a new open-weights model that could also be released around the same time, although the exact timing may vary. This would mark the first open-weights model release from OpenAI since GPT-2 in 2019.',\n",
       " 'response': 'OpenAI is expected to release GPT-5 and a new open-weights model in August 2025.',\n",
       " 'web_search': True}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc_kernel",
   "language": "python",
   "name": "arcfusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
